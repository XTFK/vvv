{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "from nlp_id.stopword import StopWord\n",
    "from nlp_id.tokenizer import Tokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Comment</th>\n",
       "      <th>LikeCount</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>stemming_ulasan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-15T15:04:49Z</td>\n",
       "      <td>@fajarhamdani1134</td>\n",
       "      <td>Sangat betul...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-13T21:18:46Z</td>\n",
       "      <td>@SuponoPono-n5g</td>\n",
       "      <td>Ternak MULYONO manaaa ngertiiii......biangnya ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['ternak', 'mulyono', 'manaa', 'ngertii', 'bia...</td>\n",
       "      <td>ternak mulyono manaa ngertii biang gak jelaass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-13T15:41:27Z</td>\n",
       "      <td>@BadariahAhmad-b7t</td>\n",
       "      <td>Kerja sawit di msia sudah middle upper class bro</td>\n",
       "      <td>0</td>\n",
       "      <td>['kerja', 'sawit', 'msia', 'middle', 'upper', ...</td>\n",
       "      <td>kerja sawit msia middle upper class saudara la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-13T09:29:59Z</td>\n",
       "      <td>@mibaa7578</td>\n",
       "      <td>Seorang yg begitu bijak.. tahniah org muda.‚ù§</td>\n",
       "      <td>0</td>\n",
       "      <td>['bijak', 'tahniah', 'org', 'muda']</td>\n",
       "      <td>bijak tahniah org muda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-13T06:34:59Z</td>\n",
       "      <td>@RobiRahmathidayat01</td>\n",
       "      <td>Parah üòÇ</td>\n",
       "      <td>0</td>\n",
       "      <td>['parah']</td>\n",
       "      <td>parah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19290</th>\n",
       "      <td>2024-10-19T11:59:14Z</td>\n",
       "      <td>@AndreCirebon</td>\n",
       "      <td>‚Äã‚Äã@@Sirimons12SMA sya juga ojol SMA kayak lu b...</td>\n",
       "      <td>2</td>\n",
       "      <td>['sya', 'ojol', 'kayak', 'bang', 'sgtu', 'hasi...</td>\n",
       "      <td>sya ojol kayak bang sgtu hasil sya phk usaha s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19291</th>\n",
       "      <td>2024-10-19T11:59:17Z</td>\n",
       "      <td>@kingki1953</td>\n",
       "      <td>No. 2 setelah pak gugem</td>\n",
       "      <td>3</td>\n",
       "      <td>['no', 'gugem']</td>\n",
       "      <td>no gugem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19292</th>\n",
       "      <td>2024-10-19T12:17:37Z</td>\n",
       "      <td>@byonechannel2419</td>\n",
       "      <td>No 2 setelah Ngomongin Uang</td>\n",
       "      <td>3</td>\n",
       "      <td>['no', 'bicara', 'uang']</td>\n",
       "      <td>no bicara uang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19293</th>\n",
       "      <td>2024-10-19T11:01:08Z</td>\n",
       "      <td>@zapz</td>\n",
       "      <td>summon</td>\n",
       "      <td>5</td>\n",
       "      <td>['summon']</td>\n",
       "      <td>summon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19294</th>\n",
       "      <td>2024-10-19T11:00:55Z</td>\n",
       "      <td>@Alqowim</td>\n",
       "      <td>Hadehh</td>\n",
       "      <td>5</td>\n",
       "      <td>['hadehh']</td>\n",
       "      <td>hadehh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19295 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Time              UserName  \\\n",
       "0      2025-05-15T15:04:49Z     @fajarhamdani1134   \n",
       "1      2025-05-13T21:18:46Z       @SuponoPono-n5g   \n",
       "2      2025-05-13T15:41:27Z    @BadariahAhmad-b7t   \n",
       "3      2025-05-13T09:29:59Z            @mibaa7578   \n",
       "4      2025-05-13T06:34:59Z  @RobiRahmathidayat01   \n",
       "...                     ...                   ...   \n",
       "19290  2024-10-19T11:59:14Z         @AndreCirebon   \n",
       "19291  2024-10-19T11:59:17Z           @kingki1953   \n",
       "19292  2024-10-19T12:17:37Z     @byonechannel2419   \n",
       "19293  2024-10-19T11:01:08Z                 @zapz   \n",
       "19294  2024-10-19T11:00:55Z              @Alqowim   \n",
       "\n",
       "                                                 Comment  LikeCount  \\\n",
       "0                                        Sangat betul...          0   \n",
       "1      Ternak MULYONO manaaa ngertiiii......biangnya ...          0   \n",
       "2       Kerja sawit di msia sudah middle upper class bro          0   \n",
       "3           Seorang yg begitu bijak.. tahniah org muda.‚ù§          0   \n",
       "4                                                Parah üòÇ          0   \n",
       "...                                                  ...        ...   \n",
       "19290  ‚Äã‚Äã@@Sirimons12SMA sya juga ojol SMA kayak lu b...          2   \n",
       "19291                            No. 2 setelah pak gugem          3   \n",
       "19292                        No 2 setelah Ngomongin Uang          3   \n",
       "19293                                             summon          5   \n",
       "19294                                             Hadehh          5   \n",
       "\n",
       "                                           preprocessing  \\\n",
       "0                                                     []   \n",
       "1      ['ternak', 'mulyono', 'manaa', 'ngertii', 'bia...   \n",
       "2      ['kerja', 'sawit', 'msia', 'middle', 'upper', ...   \n",
       "3                    ['bijak', 'tahniah', 'org', 'muda']   \n",
       "4                                              ['parah']   \n",
       "...                                                  ...   \n",
       "19290  ['sya', 'ojol', 'kayak', 'bang', 'sgtu', 'hasi...   \n",
       "19291                                    ['no', 'gugem']   \n",
       "19292                           ['no', 'bicara', 'uang']   \n",
       "19293                                         ['summon']   \n",
       "19294                                         ['hadehh']   \n",
       "\n",
       "                                         stemming_ulasan  \n",
       "0                                                    NaN  \n",
       "1      ternak mulyono manaa ngertii biang gak jelaass...  \n",
       "2      kerja sawit msia middle upper class saudara la...  \n",
       "3                                 bijak tahniah org muda  \n",
       "4                                                  parah  \n",
       "...                                                  ...  \n",
       "19290  sya ojol kayak bang sgtu hasil sya phk usaha s...  \n",
       "19291                                           no gugem  \n",
       "19292                                     no bicara uang  \n",
       "19293                                             summon  \n",
       "19294                                             hadehh  \n",
       "\n",
       "[19295 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/22.11.5169/SEM 6/PROJ DATMIN/huh/vvv/csv/Youtube-Sentimen.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PERSIAPAN DATA UNTUK INDOBERT DAN ML KLASIK**\n",
    "# Gunakan kolom 'processed_comment' yang sudah dioptimasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['processed_comment', 'label'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data_for_models = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed_comment\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# 'label' diasumsikan ground truth\u001b[39;00m\n\u001b[32m      3\u001b[39m df_train, df_test_val = train_test_split(data_for_models, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=data_for_models[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      4\u001b[39m df_val, df_test = train_test_split(df_test_val, test_size=\u001b[32m0.5\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=df_test_val[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['processed_comment', 'label'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "data_for_models = df[['processed_comment', 'label']] # 'label' diasumsikan ground truth\n",
    "\n",
    "df_train, df_test_val = train_test_split(data_for_models, test_size=0.2, random_state=42, stratify=data_for_models['label'])\n",
    "df_val, df_test = train_test_split(df_test_val, test_size=0.5, random_state=42, stratify=df_test_val['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `sentiment_indobert` for `x`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m plt.figure(figsize=(\u001b[32m6\u001b[39m,\u001b[32m4\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcountplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment_indobert\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpalette\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpastel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mDistribusi Sentimen Komentar (IndoBERT)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mSentimen\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\categorical.py:2631\u001b[39m, in \u001b[36mcountplot\u001b[39m\u001b[34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, hue_norm, stat, width, dodge, gap, log_scale, native_scale, formatter, legend, ax, **kwargs)\u001b[39m\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot pass values for both `x` and `y`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2631\u001b[39m p = \u001b[43m_CategoricalAggPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlegend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2641\u001b[39m     ax = plt.gca()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\categorical.py:67\u001b[39m, in \u001b[36m_CategoricalPlotter.__init__\u001b[39m\u001b[34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     58\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     legend=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_format == \u001b[33m\"\u001b[39m\u001b[33mwide\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_base.py:634\u001b[39m, in \u001b[36mVectorPlotter.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m._var_ordered = {\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mhue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstyle\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_base.py:679\u001b[39m, in \u001b[36mVectorPlotter.assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_format = \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m     plot_data = \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m     frame = plot_data.frame\n\u001b[32m    681\u001b[39m     names = plot_data.names\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_core\\data.py:58\u001b[39m, in \u001b[36mPlotData.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     data: DataSource,\n\u001b[32m     54\u001b[39m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[32m     55\u001b[39m ):\n\u001b[32m     57\u001b[39m     data = handle_data_source(data)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     frame, names, ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.frame = frame\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.names = names\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\taufi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_core\\data.py:232\u001b[39m, in \u001b[36mPlotData._assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    231\u001b[39m         err += \u001b[33m\"\u001b[39m\u001b[33mAn entry with this name does not appear in `data`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m \n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: Could not interpret value `sentiment_indobert` for `x`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Training data shape:', df_train.shape)\n",
    "print('Validation data shape:', df_val.shape)\n",
    "print('Test data shape:', df_test.shape)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x=df_train['label'])\n",
    "plt.title(\"Distribusi Label di Data Training\")\n",
    "plt.show()\n",
    "df_train.to_csv('data_training_indobert.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x=df_val['label'])\n",
    "plt.title(\"Distribusi Label di Data Validasi\")\n",
    "plt.show()\n",
    "df_val.to_csv('data_validasi_indobert.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.countplot(x=df_test['label'])\n",
    "plt.title(\"Distribusi Label di Data Testing\")\n",
    "plt.show()\n",
    "df_test.to_csv('data_testing_indobert.csv', index=False)\n",
    "\n",
    "# --- INDOBERT MODEL ---\n",
    "print(\"\\n--- IndoBERT Model Training & Evaluation ---\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "\n",
    "print('Kalimat contoh:', df['processed_comment'].iloc[0])\n",
    "print('BERT Tokenizer:', bert_tokenizer.tokenize(df['processed_comment'].iloc[0]))\n",
    "\n",
    "bert_input = bert_tokenizer.encode_plus(\n",
    "    df['processed_comment'].iloc[0],\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',\n",
    "    truncation='longest_first',\n",
    "    max_length=50,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=True\n",
    ")\n",
    "\n",
    "print('\\nKeys from bert_input:', bert_input.keys())\n",
    "print('Kalimat\\t\\t:', df['processed_comment'].iloc[0])\n",
    "print('Tokenizer\\t:', bert_tokenizer.convert_ids_to_tokens(bert_input['input_ids']))\n",
    "print('Input IDs\\t:', bert_input['input_ids'])\n",
    "print('Token Type IDs\\t:', bert_input['token_type_ids'])\n",
    "print('Attention Mask\\t:', bert_input['attention_mask'])\n",
    "\n",
    "token_lens = []\n",
    "for txt in df['processed_comment']:\n",
    "    tokens = bert_tokenizer.encode(txt)\n",
    "    token_lens.append(len(tokens))\n",
    "\n",
    "sns.histplot(token_lens, kde=True, stat='density', linewidth=0)\n",
    "plt.xlim([0, 100])\n",
    "plt.xlabel('Token count')\n",
    "plt.title(\"Distribusi Panjang Token Komentar\")\n",
    "plt.show()\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def convert_example_to_feature(sentence):\n",
    "    return bert_tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        truncation='longest_first',\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "    }, label\n",
    "\n",
    "def encode_dataset(data_df):\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    for sentence, label in data_df.to_numpy():\n",
    "        bert_input = convert_example_to_feature(str(sentence))\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "train_encoded = encode_dataset(df_train).batch(32)\n",
    "test_encoded = encode_dataset(df_test).batch(32)\n",
    "val_encoded = encode_dataset(df_val).batch(32)\n",
    "\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\n",
    "    'indobenchmark/indobert-base-p2', num_labels=3) # 3 kelas: negatif (0), positif (1), netral (2)\n",
    "\n",
    "bert_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00003),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy'))\n",
    "\n",
    "print(\"\\nMemulai pelatihan model IndoBERT...\")\n",
    "bert_history = bert_model.fit(train_encoded, epochs=5,\n",
    "                              batch_size=32, validation_data=val_encoded)\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.title(f\"Training and Validation {string.capitalize()}\")\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(bert_history, 'accuracy')\n",
    "plot_graphs(bert_history, 'loss')\n",
    "\n",
    "print('\\nEpoch No.  Train Accuracy   Train Loss         Val Accuracy     Val Loss')\n",
    "for i in range(len(bert_history.history['accuracy'])):\n",
    "    print('{:8d} {:10f} \\t {:10f} \\t {:10f} \\t {:10f}'.format(i + 1, bert_history.history['accuracy'][i],\n",
    "                                                             bert_history.history['loss'][i],\n",
    "                                                             bert_history.history['val_accuracy'][i],\n",
    "                                                             bert_history.history['val_loss'][i]))\n",
    "\n",
    "bert_model.save_weights('bert-model-youtube-sentiment.h5')\n",
    "print(\"\\nBobot model IndoBERT berhasil disimpan.\")\n",
    "\n",
    "print(\"\\nEvaluasi model IndoBERT pada data testing...\")\n",
    "score = bert_model.evaluate(test_encoded)\n",
    "print(f\"Test Accuracy: {score[1]:.4f}\")\n",
    "\n",
    "predicted_raw = bert_model.predict(test_encoded)\n",
    "y_pred_bert = np.argmax(predicted_raw['logits'], axis=1)\n",
    "# Ekstrak label asli dari tf.data.Dataset\n",
    "y_true_bert = np.array([label for _, label_tensor in test_encoded.unbatch() for label in label_tensor.numpy().flatten()])\n",
    "\n",
    "print(\"\\nAccuracy Score (IndoBERT):\", accuracy_score(y_true_bert, y_pred_bert))\n",
    "\n",
    "# Menampilkan Classification Report dengan label yang benar\n",
    "sentiment_labels_numeric = {0: 'negatif', 1: 'positif', 2: 'netral'}\n",
    "y_true_bert_named = np.array([sentiment_labels_numeric[val] for val in y_true_bert])\n",
    "y_pred_bert_named = np.array([sentiment_labels_numeric[val] for val in y_pred_bert])\n",
    "\n",
    "print(\"\\nClassification Report (IndoBERT):\\n\", classification_report(y_true_bert_named, y_pred_bert_named, zero_division=0))\n",
    "\n",
    "confm_bert = confusion_matrix(y_true_bert, y_pred_bert)\n",
    "columns_bert = ['negatif','positif','netral'] # Urutan ini harus cocok dengan mapping numerik 0, 1, 2\n",
    "df_cm_bert = pd.DataFrame(confm_bert, index=columns_bert, columns=columns_bert)\n",
    "ax_bert = sns.heatmap(df_cm_bert, cmap='Blues', annot=True, fmt='d')\n",
    "ax_bert.set_title('Confusion Matrix IndoBERT')\n",
    "ax_bert.set_xlabel('Label Prediksi')\n",
    "ax_bert.set_ylabel('Label Sebenarnya')\n",
    "plt.show()\n",
    "\n",
    "# --- TRADITIONAL MACHINE LEARNING MODEL COMPARISON ---\n",
    "print(\"\\n--- Traditional Machine Learning Model Comparison ---\")\n",
    "\n",
    "# Persiapan data untuk ML Klasik\n",
    "# Menggunakan processed_comment sebagai fitur teks dan 'label' sebagai target\n",
    "X_train_ml = df_train['processed_comment']\n",
    "y_train_ml = df_train['label']\n",
    "X_test_ml = df_test['processed_comment']\n",
    "y_test_ml = df_test['label']\n",
    "\n",
    "# Inisialisasi TF-IDF Vectorizer\n",
    "# Anda bisa menyesuaikan parameter seperti max_features, min_df, max_df\n",
    "vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.8)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train_ml)\n",
    "X_test_vectorized = vectorizer.transform(X_test_ml)\n",
    "\n",
    "# Simpan vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer_ml.pkl')\n",
    "\n",
    "models = {\n",
    "    \"LinearSVC\": LinearSVC(random_state=42, max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, n_estimators=100),\n",
    "    \"SGD Classifier\": SGDClassifier(random_state=42, max_iter=1000, tol=1e-3),\n",
    "    \"MLP Classifier\": MLPClassifier(random_state=42, max_iter=300),\n",
    "    \"Dummy Classifier\": DummyClassifier(strategy=\"most_frequent\")\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Evaluasi Model: {name} ---\")\n",
    "    model.fit(X_train_vectorized, y_train_ml)\n",
    "    y_pred_ml = model.predict(X_test_vectorized)\n",
    "\n",
    "    acc = accuracy_score(y_test_ml, y_pred_ml)\n",
    "    prec = precision_score(y_test_ml, y_pred_ml, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test_ml, y_pred_ml, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_ml, y_pred_ml, average='weighted', zero_division=0)\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-Score\": f1\n",
    "    }\n",
    "\n",
    "    # Menampilkan Classification Report\n",
    "    sentiment_labels_numeric_ml = {0: 'negatif', 1: 'positif', 2: 'netral'}\n",
    "    y_true_ml_named = np.array([sentiment_labels_numeric_ml[val] for val in y_test_ml])\n",
    "    y_pred_ml_named = np.array([sentiment_labels_numeric_ml[val] for val in y_pred_ml])\n",
    "\n",
    "    print(classification_report(y_true_ml_named, y_pred_ml_named, zero_division=0))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm_ml = confusion_matrix(y_test_ml, y_pred_ml, labels=[0, 1, 2]) # Sesuaikan dengan mapping numerik Anda\n",
    "    df_cm_ml = pd.DataFrame(cm_ml, index=['negatif', 'positif', 'netral'], columns=['negatif', 'positif', 'netral'])\n",
    "    sns.heatmap(df_cm_ml, cmap='Blues', annot=True, fmt='d')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.xlabel('Label Prediksi')\n",
    "    plt.ylabel('Label Sebenarnya')\n",
    "    plt.show()\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='F1-Score', ascending=False)\n",
    "print(\"\\nPerbandingan Performa Model Klasifikasi:\")\n",
    "from IPython.display import display # Perlu import ini jika ingin display() bekerja di non-Jupyter notebook\n",
    "display(results_df.style.background_gradient(cmap='Blues').format(\"{:.2%}\"))\n",
    "\n",
    "# --- TRAINING DAN SIMPAN MODEL FINAL (CONTOH: DECISION TREE) ---\n",
    "# Anda bisa memilih model terbaik dari hasil perbandingan di atas\n",
    "print(\"\\n--- Training Final Model (contoh: Decision Tree) ---\")\n",
    "final_ml_model = DecisionTreeClassifier(random_state=42) # Ganti dengan model terbaik Anda jika berbeda\n",
    "final_ml_model.fit(X_train_vectorized, y_train_ml)\n",
    "\n",
    "y_pred_final_ml = final_ml_model.predict(X_test_vectorized)\n",
    "\n",
    "print(classification_report(y_test_ml, y_pred_final_ml, zero_division=0))\n",
    "\n",
    "cm_final_ml = confusion_matrix(y_test_ml, y_pred_final_ml, labels=final_ml_model.classes_)\n",
    "\n",
    "df_cm_final_ml = pd.DataFrame(cm_final_ml, index=['negatif', 'positif', 'netral'], columns=['negatif', 'positif', 'netral'])\n",
    "sns.heatmap(df_cm_final_ml, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Final ML Model (Decision Tree)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Akurasi             : {accuracy_score(y_test_ml, y_pred_final_ml):.2f}\")\n",
    "print(f\"Precision (Weighted): {precision_score(y_test_ml, y_pred_final_ml, average='weighted', zero_division=0):.2f}\")\n",
    "print(f\"Recall (Weighted)   : {recall_score(y_test_ml, y_pred_final_ml, average='weighted', zero_division=0):.2f}\")\n",
    "print(f\"F1-Score (Weighted) : {f1_score(y_test_ml, y_pred_final_ml, average='weighted', zero_division=0):.2f}\")\n",
    "\n",
    "joblib.dump(final_ml_model, 'sentiment_ml_model.pkl') # Beri nama file yang jelas\n",
    "print(\"\\nModel ML Klasik final berhasil disimpan.\")\n",
    "\n",
    "\n",
    "# --- FUNGSI PREDIKSI UNTUK DEPLOYMENT SIMULASI ---\n",
    "# Load kembali model dan vectorizer (simulasi deployment)\n",
    "loaded_ml_model = joblib.load('sentiment_ml_model.pkl')\n",
    "loaded_vectorizer = joblib.load('vectorizer_ml.pkl')\n",
    "\n",
    "# Fungsi prediksi menggunakan model ML Klasik\n",
    "def predict_sentiment_ml_inference(comment):\n",
    "    # Pra-proses komentar menggunakan fungsi yang sama\n",
    "    processed_comment = my_full_text_preprocessor_optimized(comment)\n",
    "    # Vektorisasi komentar\n",
    "    vect_comment = loaded_vectorizer.transform([processed_comment])\n",
    "    # Prediksi sentimen numerik\n",
    "    pred_numeric = loaded_ml_model.predict(vect_comment)[0]\n",
    "    # Konversi numerik ke label teks\n",
    "    sentiment_map = {0: 'negatif', 1: 'positif', 2: 'netral'}\n",
    "    return sentiment_map[pred_numeric]\n",
    "\n",
    "# Fungsi prediksi menggunakan model IndoBERT\n",
    "def predict_sentiment_bert_inference(comment):\n",
    "    processed_comment = my_full_text_preprocessor_optimized(comment)\n",
    "    # Tokenisasi dan encode untuk BERT\n",
    "    bert_input = convert_example_to_feature(processed_comment)\n",
    "    input_dict = {\n",
    "        \"input_ids\": tf.constant([bert_input['input_ids']], dtype=tf.int32),\n",
    "        \"token_type_ids\": tf.constant([bert_input['token_type_ids']], dtype=tf.int32),\n",
    "        \"attention_mask\": tf.constant([bert_input['attention_mask']], dtype=tf.int32)\n",
    "    }\n",
    "    # Prediksi\n",
    "    outputs = bert_model(input_dict)\n",
    "    logits = outputs.logits\n",
    "    pred_numeric = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    sentiment_map = {0: 'negatif', 1: 'positif', 2: 'netral'}\n",
    "    return sentiment_map[pred_numeric]\n",
    "\n",
    "print(\"\\n--- Simulasi Prediksi Komentar Baru ---\")\n",
    "user_input = input(\"Masukkan komentar YouTube: \")\n",
    "\n",
    "# Prediksi menggunakan model ML Klasik\n",
    "result_ml_inf = predict_sentiment_ml_inference(user_input)\n",
    "print(f\"\\nSentimen prediksi (ML Klasik): {result_ml_inf}\")\n",
    "\n",
    "# Prediksi menggunakan model IndoBERT\n",
    "result_bert_inf = predict_sentiment_bert_inference(user_input)\n",
    "print(f\"Sentimen prediksi (IndoBERT): {result_bert_inf}\")\n",
    "\n",
    "# Prediksi menggunakan model Leksikon\n",
    "result_lexicon_inf = prediksiSentiment_lexicon_advanced(\n",
    "    my_full_text_preprocessor_optimized(user_input),\n",
    "    sentiment_lexicon, all_negation_words, intensifiers, reducers\n",
    ")\n",
    "print(f\"Sentimen prediksi (Leksikon): {result_lexicon_inf}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
